---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

Hi, Iâ€™m Run Luo. I am currently a third-year Master student in Computer Technology at the University of Chinese Academy of Sciences (UCAS), supervised by Prof. [Min Yang](https://minyang.me/). I received my Bachelor's degree in Software Engineering from Huazhong University of Science and Technology (HUST) in 2023.  My research interests focus on Visual Tracking, Diffusion Model, Multi-modal Learning, and Large Language Models. I am currently exploring a unified omnimodal foundational model involving vision, audio, and text modalities, and I hope to see the model generate synergy and benefit from both generation and understanding, thereby extending the intelligence boundaries of existing models. I firmly believe that it can unify the paradigms of world models or vision-language-action models, and through this, benefit interactions across different physical devices and the real world.


# ğŸ”¥ News
- *2026.01*: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by ICLR 2026.
- *2025.09*: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by NeurIPS 2025.
- *2025.05*: &nbsp;ğŸ‰ğŸ‰ Two papers are accepted by ACL 2025.
- *2025.01*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by ICLR 2025.
- *2024.11*: &nbsp;ğŸ‰ğŸ‰ Two paper is accepted by AAAI 2024.
- *2024.07*: &nbsp;ğŸ‰ğŸ‰ One paper is accepted by EMNLP 2024.
- *2024.04*: &nbsp;ğŸ‰ğŸ‰ Join **Tongyi Lab@Beijing** for a internship.
- *2024.04*: ğŸ‰ğŸ‰ Two papers are accepted by ACL 2024.
- *2023.11*: ğŸ‰ğŸ‰ One paper is accepted by AAAI 2023.
- *2022.11*: ğŸ‰ğŸ‰ One paper is accepted by AAAI 2022.

# ğŸ“ Selected Publications 
\* indicates equal contribution
- NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching. **ICLR 2026**.<br>
**Run Luo**, Xiaobo Xia, Lu Wang, Longze Chen, Renke Shan, Jing Luo, Min Yang, Tat-Seng Chua <br>
[[Paper]](https://arxiv.org/abs/2510.13721) [Code (coming soon)]
- GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents. **Arxiv**<br>
**Run Luo**, Lu Wang, Wanwei He, Longze Chen, Jiaming Li, Min Yang, Xiaobo Xia <br>
[[Paper]](https://arxiv.org/abs/2504.10458) [[Code]](https://github.com/ritzz-ai/GUI-R1)
- OpenOmni: Advancing Open-Source Omnimodal Large Language Models with Progressive Multimodal Alignment and Real-Time Self-Aware Emotional Speech Synthesis. **NeurIPS 2025**. <br>
**Run Luo**, Ting-En Lin, Haonan Zhang, Yuchuan Wu, Xiong Liu, Min Yang, Yongbin Li, Longze Chen, Jiaming Li, Lei Zhang, Xiaobo Xia, Hamid Alinejad-Rokny, Fei Huang <br>
[[Paper]](https://arxiv.org/pdf/2501.04561) [[Code]](https://github.com/RainBowLuoCS/OpenOmni)
- VCM: Vision Concept Modeling with Adaptive Vision Token Compression. **NeurIPS 2025**. <br>**Run Luo**, Renke Shan, Longze Chen, Ziqiang Liu, Lu Wang, Min Yang, Xiaobo Xia <br>
[[Paper]](https://arxiv.org/abs/2504.19627) 
- OmniCharacter: Towards Immersive Role-Playing Agents with Seamless Speech-Language Personality Interaction. **ACL 2025**. <br>
Haonan Zhang*, **Run Luo\***, Xiong Liu, Yuchuan Wu, Ting-En Lin, Pengpeng Zeng, QIANG QU, Feiteng Fang, Min Yang, Lianli Gao, Jingkuan Song, Fei Huang, Yongbin Li<br>
[[Paper]](https://www.arxiv.org/pdf/2505.20277) [[Code]](https://github.com/zchoi/OmniCharacter)
- MMEvol: Empowering Multimodal Large Language Models with Evol-Instruct. **ACL 2025ï¼ˆFindingsï¼‰**.  <br>
**Run Luo**, Haonan Zhang, Longze Chen, Ting-En Lin, Xiong Liu, Yuchuan Wu, Min Yang, Minzheng Wang, Pengpeng Zeng, Lianli Gao, Heng Tao Shen, Yunshui Li, Xiaobo Xia, Fei Huang, Jingkuan Song, Yongbin Li <br>
[[Project Page]](https://mmevol.github.io/home_page.html) [[Paper]](https://arxiv.org/pdf/2409.05840) [[Code]](https://github.com/RainBowLuoCS/MMEvol)
- DEEM: Diffusion models serve as the eyes of large language models for image perception. **ICLR 2025** **Spotlight**. <br>
**Run Luo**, Yunshui Li, Longze Chen, Wanwei He, Ting-En Lin, Ziqiang Liu, Lei Zhang, Zikai Song, Xiaobo Xia, Tongliang Liu, Min Yang, Binyuan Hui <br>
[[Paper]](https://arxiv.org/abs/2405.15232) [[Code]](https://github.com/RainBowLuoCS/DEEM)
- DiffusionTrack: Diffusion Model For Multi-Object Tracking. **AAAI 2023**. <br>
**Run Luo**, Zikai Song, Lintao Ma, Jinlin Wei, Wei Yang, Min Yang<br>
[[Paper]](https://arxiv.org/abs/2308.09905) [[Code]](https://github.com/RainBowLuoCS/DiffusionTrack)
- Compact Transformer Tracker with Correlative Masked Modeling. **AAAI 2022**. <br>Zikai Song\*, **Run Luo\***, Junqing Yu, Chen Yi-Ping Phoebe, Wei Yang<br>
[[Paper]](https://arxiv.org/abs/2301.10938) [[Code]](https://github.com/HUSTDML/CTTrack)

# ğŸ– Honors and Scholarships
- 2025.10 National Scholarship.

# ğŸ“– Educations
- 2023.09 - 2026.06, **University of Chinese Academy of Sciences**, Master of Computer Technology.
- 2019.09 - 2023.06, **Huazhong University of Science and Technology**, Bachelor of Software Engineering.

# ğŸ’» Internships
- *2024.04 - 2024.11*, [Tongyi Lab](https://careers-tongyi.alibaba.com/home?lang=zh), Alibaba Group, China.

# ğŸ’¬ Services
 Reviewer for CVPR23-25, ICCV25, ICLR25-26, ICML25, NIPS24-25, AAAI22-26, ACL24-25, EMNLP24-25, ACM MM 23-25 etc.

<a href="https://www.easycounter.com/">
<img src="https://www.easycounter.com/counter.php?runluo"
border="0" alt="Free Web Counters"></a> 

visitors since Oct. 2025

<a href="https://clustrmaps.com/site/1c84s"  title="ClustrMaps"><img src="//www.clustrmaps.com/map_v2.png?d=sSqg5mWL7c8Y6ZbC3SlJNgnYZ94lgifrRe0U1CxnI70&cl=ffffff" /></a>

